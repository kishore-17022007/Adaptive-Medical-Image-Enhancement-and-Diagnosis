{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# PNEUMONIA DETECTION FROM CHEST X-RAYS - IMPROVED WORKING CODE\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.utils import class_weight\n",
        "import os\n",
        "\n",
        "# 1. SETUP ======================================================\n",
        "IMG_SIZE = (224, 224)  # ResNet input size\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 15  # Increased epochs to allow for early stopping\n",
        "LR = 0.0001\n",
        "\n",
        "# 2. DATA PIPELINE ==============================================\n",
        "base_dir = '/content/chest_xray' # Corrected base directory\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Add checks for directory existence and content\n",
        "print(f\"Checking if {base_dir} exists: {os.path.exists(base_dir)}\")\n",
        "if os.path.exists(base_dir):\n",
        "    print(f\"Contents of {base_dir}: {os.listdir(base_dir)}\")\n",
        "    print(f\"Checking if {train_dir} exists: {os.path.exists(train_dir)}\")\n",
        "    if os.path.exists(train_dir):\n",
        "        print(f\"Contents of {train_dir}: {os.listdir(train_dir)}\")\n",
        "    print(f\"Checking if {val_dir} exists: {os.path.exists(val_dir)}\")\n",
        "    if os.path.exists(val_dir):\n",
        "        print(f\"Contents of {val_dir}: {os.listdir(val_dir)}\")\n",
        "    print(f\"Checking if {test_dir} exists: {os.path.exists(test_dir)}\")\n",
        "    if os.path.exists(test_dir):\n",
        "        print(f\"Contents of {test_dir}: {os.listdir(test_dir)}\")\n",
        "else:\n",
        "    print(\"Base directory does not exist. Please ensure the dataset is unzipped correctly.\")\n",
        "\n",
        "\n",
        "# Enhanced data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    shear_range=0.15,\n",
        "    zoom_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,  # Added vertical flip\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create generators with balanced classes\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=True,\n",
        "    seed=42  # Fixed seed for reproducibility\n",
        ")\n",
        "\n",
        "val_generator = test_val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = test_val_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Calculate class weights to handle imbalance\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(f\"Class weights: {class_weights}\")  # Debug output\n",
        "\n",
        "# 3. IMPROVED MODEL BUILDING ====================================\n",
        "base_model = ResNet50(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
        "    pooling='avg'  # Directly add global pooling\n",
        ")\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "# Simplified model architecture\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
        "    base_model,\n",
        "    layers.Dropout(0.3),  # Reduced dropout\n",
        "    layers.Dense(128, activation='relu'),  # Smaller dense layer\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Mixed precision training for better performance\n",
        "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(LR),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "        tf.keras.metrics.AUC(name='auc'),\n",
        "        tf.keras.metrics.TruePositives(name='tp'),\n",
        "        tf.keras.metrics.FalsePositives(name='fp')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 4. ENHANCED TRAINING ==========================================\n",
        "callbacks = [\n",
        "    callbacks.EarlyStopping(\n",
        "        monitor='val_auc',  # Changed to monitor AUC\n",
        "        patience=5,\n",
        "        mode='max',\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=2,\n",
        "        min_lr=1e-7\n",
        "    ),\n",
        "    callbacks.ModelCheckpoint(\n",
        "        'best_model.h5',\n",
        "        monitor='val_auc',\n",
        "        save_best_only=True,\n",
        "        mode='max'\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,  # Added class weights\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# 5. COMPREHENSIVE EVALUATION ===================================\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "    plt.title('Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    # AUC\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(history.history['auc'], label='Train')\n",
        "    plt.plot(history.history['val_auc'], label='Validation')\n",
        "    plt.title('AUC')\n",
        "    plt.ylabel('AUC')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(history.history['loss'], label='Train')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.title('Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "# Load best model for final evaluation\n",
        "model = models.load_model('best_model.h5')\n",
        "\n",
        "# Test evaluation\n",
        "test_results = model.evaluate(test_generator)\n",
        "print(\"\\nFINAL TEST METRICS:\")\n",
        "print(f\"Accuracy: {test_results[1]:.4f}\")\n",
        "print(f\"Precision: {test_results[2]:.4f}\")\n",
        "print(f\"Recall: {test_results[3]:.4f}\")\n",
        "print(f\"AUC: {test_results[4]:.4f}\")\n",
        "print(f\"True Positives: {test_results[5]}\")\n",
        "print(f\"False Positives: {test_results[6]}\")\n",
        "\n",
        "# Enhanced confusion matrix\n",
        "test_generator.reset()\n",
        "y_true = test_generator.classes\n",
        "y_pred = model.predict(test_generator) > 0.5\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Normal', 'Pneumonia'],\n",
        "            yticklabels=['Normal', 'Pneumonia'],\n",
        "            annot_kws={\"size\": 16})\n",
        "plt.title('Confusion Matrix', fontsize=14)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nCLASSIFICATION REPORT:\")\n",
        "print(classification_report(y_true, y_pred, target_names=['Normal', 'Pneumonia']))\n",
        "\n",
        "# 6. MODEL SAVING ===============================================\n",
        "model.save('pneumonia_detection.h5')\n",
        "print(\"Model saved as pneumonia_detection.h5\")\n",
        "\n",
        "# Save the class indices for deployment\n",
        "import json\n",
        "class_indices = train_generator.class_indices\n",
        "with open('class_indices.json', 'w') as f:\n",
        "    json.dump(class_indices, f)\n",
        "print(\"Class indices saved to class_indices.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKR2sacEWwqp",
        "outputId": "2ef2f255-bcf4-4094-f52e-a424567ddc79"
      },
      "id": "uKR2sacEWwqp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking if /content/chest_xray exists: True\n",
            "Contents of /content/chest_xray: ['val', 'train', 'test']\n",
            "Checking if /content/chest_xray/train exists: True\n",
            "Contents of /content/chest_xray/train: ['NORMAL', 'PNEUMONIA']\n",
            "Checking if /content/chest_xray/val exists: True\n",
            "Contents of /content/chest_xray/val: ['NORMAL', 'PNEUMONIA']\n",
            "Checking if /content/chest_xray/test exists: True\n",
            "Contents of /content/chest_xray/test: ['NORMAL', 'PNEUMONIA']\n",
            "Found 5216 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n",
            "Class weights: {0: np.float64(1.9448173005219984), 1: np.float64(0.6730322580645162)}\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163/163 - 696s - 4s/step - accuracy: 0.5015 - auc: 0.5130 - fp: 634.0000 - loss: 0.7112 - precision: 0.7507 - recall: 0.4926 - tp: 1909.0000 - val_accuracy: 0.6250 - val_auc: 0.8516 - val_fp: 6.0000 - val_loss: 0.6777 - val_precision: 0.5714 - val_recall: 1.0000 - val_tp: 8.0000 - learning_rate: 1.0000e-04\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163/163 - 690s - 4s/step - accuracy: 0.5684 - auc: 0.6072 - fp: 551.0000 - loss: 0.6759 - precision: 0.7979 - recall: 0.5613 - tp: 2175.0000 - val_accuracy: 0.5000 - val_auc: 0.8594 - val_fp: 0.0000e+00 - val_loss: 0.6800 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_tp: 0.0000e+00 - learning_rate: 1.0000e-04\n",
            "Epoch 3/15\n",
            "163/163 - 754s - 5s/step - accuracy: 0.6246 - auc: 0.6840 - fp: 457.0000 - loss: 0.6521 - precision: 0.8386 - recall: 0.6126 - tp: 2374.0000 - val_accuracy: 0.8125 - val_auc: 0.8516 - val_fp: 0.0000e+00 - val_loss: 0.6474 - val_precision: 1.0000 - val_recall: 0.6250 - val_tp: 5.0000 - learning_rate: 1.0000e-04\n",
            "Epoch 4/15\n",
            "163/163 - 701s - 4s/step - accuracy: 0.6495 - auc: 0.7138 - fp: 416.0000 - loss: 0.6382 - precision: 0.8555 - recall: 0.6356 - tp: 2463.0000 - val_accuracy: 0.8125 - val_auc: 0.8594 - val_fp: 0.0000e+00 - val_loss: 0.6336 - val_precision: 1.0000 - val_recall: 0.6250 - val_tp: 5.0000 - learning_rate: 1.0000e-04\n",
            "Epoch 5/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163/163 - 704s - 4s/step - accuracy: 0.6856 - auc: 0.7434 - fp: 417.0000 - loss: 0.6234 - precision: 0.8641 - recall: 0.6844 - tp: 2652.0000 - val_accuracy: 0.8125 - val_auc: 0.8672 - val_fp: 2.0000 - val_loss: 0.6154 - val_precision: 0.7778 - val_recall: 0.8750 - val_tp: 7.0000 - learning_rate: 1.0000e-04\n",
            "Epoch 6/15\n",
            "163/163 - 726s - 4s/step - accuracy: 0.7136 - auc: 0.7812 - fp: 411.0000 - loss: 0.6004 - precision: 0.8717 - recall: 0.7205 - tp: 2792.0000 - val_accuracy: 0.7500 - val_auc: 0.8672 - val_fp: 2.0000 - val_loss: 0.6009 - val_precision: 0.7500 - val_recall: 0.7500 - val_tp: 6.0000 - learning_rate: 1.0000e-04\n",
            "Epoch 7/15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For very large datasets (avoids re-uploading)\n",
        "if not os.path.exists(\"/content/chest_xray\"):\n",
        "    !unzip \"/content/drive/MyDrive/rep/chest_xray.zip\" -d \"/content\"\n",
        "else:\n",
        "    print(\"Dataset already extracted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEVGNn3ib_Ow",
        "outputId": "10c4b94e-f5af-41db-d59a-a57cb3304d23"
      },
      "id": "XEVGNn3ib_Ow",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/rep/chest_xray.zip, /content/drive/MyDrive/rep/chest_xray.zip.zip or /content/drive/MyDrive/rep/chest_xray.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check zip contents without extracting\n",
        "!unzip -l \"/content/drive/MyDrive/rep/chest_xray.zip\"\n",
        "\n",
        "# Fix permission issues\n",
        "!chmod -R 755 \"/content/chest_xray\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZqMPkuccCTX",
        "outputId": "d8576276-722d-4059-e987-803d9c1616f0"
      },
      "id": "aZqMPkuccCTX",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/rep/chest_xray.zip, /content/drive/MyDrive/rep/chest_xray.zip.zip or /content/drive/MyDrive/rep/chest_xray.zip.ZIP.\n",
            "chmod: cannot access '/content/chest_xray': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Unzip dataset (run only once)\n",
        "!unzip -q \"/content/drive/MyDrive/rep/chest_xray.zip\" -d \"/content\"\n",
        "\n",
        "# Verify dataset structure\n",
        "base_dir = '/content/chest_xray'\n",
        "required_folders = ['train', 'val', 'test']\n",
        "required_classes = ['NORMAL', 'PNEUMONIA']\n",
        "\n",
        "print(\"üîç Verifying dataset structure...\")\n",
        "for folder in required_folders:\n",
        "    folder_path = os.path.join(base_dir, folder)\n",
        "    if not os.path.exists(folder_path):\n",
        "        raise FileNotFoundError(f\"Missing folder: {folder_path}\")\n",
        "\n",
        "    for class_name in required_classes:\n",
        "        class_path = os.path.join(folder_path, class_name)\n",
        "        if not os.path.exists(class_path):\n",
        "            raise FileNotFoundError(f\"Missing class folder: {class_path}\")\n",
        "\n",
        "        num_images = len(os.listdir(class_path))\n",
        "        print(f\"‚úÖ {folder}/{class_name}: {num_images} images\")\n",
        "\n",
        "print(\"\\nüéâ Dataset structure verified successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8N8emJca_7P",
        "outputId": "40694976-777a-4a3a-f76b-60bc11faa3ca"
      },
      "id": "E8N8emJca_7P",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üîç Verifying dataset structure...\n",
            "‚úÖ train/NORMAL: 1341 images\n",
            "‚úÖ train/PNEUMONIA: 3875 images\n",
            "‚úÖ val/NORMAL: 8 images\n",
            "‚úÖ val/PNEUMONIA: 8 images\n",
            "‚úÖ test/NORMAL: 234 images\n",
            "‚úÖ test/PNEUMONIA: 390 images\n",
            "\n",
            "üéâ Dataset structure verified successfully!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}